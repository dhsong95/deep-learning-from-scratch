4장: 신경망 학습
=============

1. Loss Function 은 현재 신경망이 얼마나 데이터를 틀리는지를 수량화한 함수
   1. 대표적인 Loss Function으로는 MSE(Mean Squared Error)와 Cross Entropy가 있다
   2. Loss Function의 핵심은 틀릴 수록 함수의 큰 결과가 나와야한다
2. 손실함수를 통해 가중치(매개변수)가 결과에 미치는 영향력(미분)을 구하고, 미분값을 통해 가중치(매개변수)를 조절해서 Loss Function의 결과가 작아지도록 한다
   1. Loss Function을 작아지게 만드는 가중치(매개변수)를 찾는 것이 신경망 학습
   2. 만약 활성화함수로 계단함수를 사용할 경우 손실함수는 기울기가 0인 구간이 생긴다. 기울기가 0인 구간에서는 가중치(매개변수)를 어떠한 방향으로도 이동하지 않으므로 학습이 이루어지지 않는다. 따라서 신경망에서는 퍼셉트론과 달리 계단 함수가 아닌 시그모이드 함수를 활성함수로 사용한다
3. 결국 중요한 것은 미분
   1. 특정 지점에서의 미분은 그 지점보다 아주 조금 양의 방향으로 이동했을 때 함수의 결과가 어떻게 되는지를 파악하는 것이다.
   2. 수치 미분은 해석적 미분과 다르게 아주 작은 값에 대한 함수값을 통해 미분값을 근사한다
   3. 편미분은 관심을 가지는 대상을 제외하고 나머지 변수를 상수화해서 구한다
4. 그라디언트는 무엇인가?
   1. 함수가 다변수로 이루어져있고, 다변수 각각의 미분을 편미분을 통해 계산한 결과를 그라디언트(기울기)라고 한다
   2. 특정 변수의 그라디언트를 구하고, 그라디언트의 반대방향으로 이동하면 함수의 값을 작게 만들 수 있다 -> 경사하강법(Gradient Descent)의 핵심
5. Gradient Descent
   1. 현재 가중치(매개변수)에 대한 Loss Function을 구한다.
   2. Loss Function에 대한 각 가중치의 미분값을 구한다. Loss Function에 대한 각 가중치의 미분값은, 가중치를 오른쪽으로 조금 이동시켰을 경우 Loss Function이 얼마만큼 변하는 지를 나타낸다.
   3. 가중치를 구한 미분값의 반대방향으로 이동한다. 또한 Learning Rate를 통해서 크기를 조절한다. 적당한 Learning Rate가 학습이 잘 이루어지도록 하는 핵심요소이다.
6. 주의: 실제로 코드를 수행하다보면 엄청나게 오래걸립니다. [\[코드\]](./train_neuralnet.py)
   1. 미분값을 일일이 계산하는 것을 많은 시간을 요한다.
   2. 오차역전파법을 통해 다음장에서 개선